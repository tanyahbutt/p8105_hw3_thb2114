---
title: "P8105 Homework 3 - Tanya Butt (thb2114)"
output: github_document
---

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(lubridate)
library(p8105.datasets)
library(readxl)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1
First, I will load the the Instacart data from the p8105.datasets library and
evaluate its contents.

```{r, eval = FALSE}
data("instacart")
force(instacart)
instacart %>% 
  count(user_id)
```

The Instacart data is an online grocery shopping dataset from 2017. It includes
the following `r ncol(instacart)` variables: `r names(instacart)`. 

The data has `r nrow(instacart)` observations. Each row is a product from a 
single order from `r n_distinct(pull(instacart, user_id))` unique individuals.

I will now produce some graphs using the Instacart data. 

```{r, collapse = TRUE}
instacart %>% 
  count(department, name = "department_id") %>% 
  ggplot(aes(x = department, y = department_id)) +
  geom_point() +
  labs(
    title = "Number of Products Ordered by Department ",
    x = "Department",
    y = "Number of Prodcuts Ordered",
    caption =  "Data from Instacart"
  ) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_continuous(labels = scales::comma)
```

The departments most frequently ordered from in this dataset set are "produce"
and "dairy eggs".

```{r, collapse = TRUE}
instacart %>% 
  group_by(department, product_name) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 5000) %>% 
  ggplot(aes(x = product_name, y = n_obs)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Top 10 Products Ordered in the Instacart Dataset",
    x = "Product Name",
    y = "Number of Prodcuts Ordered",
    caption =  "Data from Instacart"
  ) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_continuous(
    breaks = c(0, 5000, 10000, 15000, 20000),
    labels = c("0", "5,000", "10,000", "15,000", "20,000")) +
  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 10))

```

The top 10 most frequently ordered products in the Instacart dataset are bags 
of organic bananas, bananas, large lemons, limes, organic avocados, organic 
baby spinach, organic Has avocados, organic raspberries, organic strawberries,
and strawberries.
 
```{r, collapse = TRUE}
n_distinct(pull(instacart, aisle_id))

instacart %>% 
  count(aisle_id, aisle, sort = TRUE)
```

In the Instacart dataset, the number of aisles are `r n_distinct(pull(instacart,aisle_id))` and the aisles most ordered from are the 'fresh vegetables' and 'fresh fruits'
aisle.

Below is a bar graph of the number of items ordered in aisles with > 10,000 
items ordered. 

```{r}
instacart %>% 
  count(aisle_id, aisle, sort = TRUE) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Number of Products Ordered",
    x = "Aisle Name",
    y = "Number of Prodcuts Ordered",
    caption =  "Data from Instacart"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1.05, size = 8)) +
  scale_y_continuous(
    breaks = c(0, 10000, 20000, 40000, 60000, 80000, 100000, 120000, 140000,
               160000),
    labels = c("0", "10,000", "20,000", "40,000", "60,000", "80,000", "100,000", 
               "120,000", "140,000", "160,000")) 
```

'Fresh vegetables' and 'fresh fruits' had the highest number 
of orders in this dataset.

Below I will create a table with the three most popular items in each of the 
aisles, "baking ingredients", "dog food care",  and "packaged vegetable fruits." 
I will include the number of times each item is ordered in the table.

```{r, collapse = TRUE}
baking_df = filter(instacart, aisle == "baking ingredients") %>% 
  group_by(aisle, product_name) %>% 
  count(product_name, sort = TRUE) %>% 
  filter(product_name == "Light Brown Sugar" | product_name == "Pure Baking Soda" |
         product_name == "Cane Sugar") 

dogfood_df = filter(instacart, aisle == "dog food care") %>% 
  group_by(aisle, product_name) %>% 
  count(product_name, sort = TRUE) %>% 
  filter(product_name == "Snack Sticks Chicken & Rice Recipe Dog Treats" | 
           product_name == "Organix Chicken & Brown Rice Recipe" | product_name
         == "Small Dog Biscuits")

package_vege_fruit_df = filter(instacart, aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  count(product_name, sort = TRUE) %>% 
  filter(product_name == "Organic Baby Spinach" | product_name == "Organic Raspberries" |
           product_name == "Organic Blueberries")

package_baking_df = full_join(package_vege_fruit_df, baking_df, by = NULL) 

baking_dogfood_package_df = full_join(package_baking_df, dogfood_df, by = NULL) %>%
  group_by(aisle) %>% 
  mutate(n, rank = min_rank(desc(n))) %>%
  mutate(product_order_num = paste(product_name,"- number of orders:", n)) 

table_df = select(baking_dogfood_package_df, rank, product_order_num, aisle) %>% 
  pivot_wider(
    names_from = aisle,
    values_from = product_order_num) %>% 
  rename("Rank" = rank) %>% 
  rename("Packaged Vegetables and Fruits" = `packaged vegetables fruits`) %>% 
  rename("Baking Ingredients" = `baking ingredients`) %>% 
  rename("Dog Food and Care" = `dog food care`) %>% 
   knitr::kable()

table_df
```

The most ordered items in the "Packaged Vegetables and Fruits", "Baking 
Ingredients", and "Dog Food and Care" aisles are: organic baby spinach, 
light brown sugar, and Snack Sticks Chicken and Rice Recipe Dog Treats.

In the following code, I am creating a table to show the mean hour of the day 
at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r, collapse = TRUE}
PLA_CIC_df = filter(instacart, product_name == "Pink Lady Apples" | product_name
                    == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_order_hour = mean(order_hour_of_day)) %>% 
  rename("Name of Product" = product_name) %>% 
  rename("Mean Order Hour" = mean_order_hour) %>%
  mutate(order_dow = factor(c("0" = "Sunday", "1" = "Monday", "2" = "Tuesday",
                              "3" = "Wednesday", "4" = "Thursday", "5" = "Friday",
                              "6" = "Saturday"))) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "Mean Order Hour"
  ) %>% 
  knitr::kable()
  
PLA_CIC_df        
```

Based on the table, it looks like Pink Lady Apples tend to be ordered earlier 
in the day compared to Coffee Ice Cream.

## Problem 2
I will load and clean data from the "Behavioral Risk Factors Surveillance 
System (BRFSS) for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010".

```{r, collapse = TRUE}
data("brfss_smart2010")
brfss_smart2010_df = force(brfss_smart2010) %>% 
 janitor::clean_names() %>% 
  rename(state_abbrev = locationabbr) %>%
  rename(state_w_county = locationdesc) %>% 
  rename(response_id = respid) %>% 
  rename(lat_long = geo_location) %>% 
  filter(topic == "Overall Health") %>% 
  filter(response == "Poor" | response == "Fair" | response == "Good" | 
           response == "Very good" | response == "Excellent") %>% 
  mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good",
                                                     "Very good", "Excellent")))
```

I will now show how many states had 7 or more locations in 2002 and 2010.

```{r, collapse = TRUE}
brfss_smart2010_df %>% 
  filter(year == 2002) %>% 
  count(state_abbrev, sort = TRUE) %>% 
  filter(n >= 7)

brfss_smart2010_df %>% 
  filter(year == 2010) %>% 
  count(state_abbrev, sort = TRUE) %>% 
  filter(n >= 7)
```

In 2002, there were 36 states with 7 or more locations. In 2010, there were 45
states
with 7 or more locations.

I will now construct a dataset limited to "Excellent" responses, contains year,
state, and a variable that averages the data value across locations within a state.

I will then make a "spaghetti" plot of the average data value over time within 
a state.

```{r, collapse = TRUE}
brfss_smart2010_df %>% 
  select(year, state_abbrev, state_w_county, response, data_value) %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state_abbrev) %>% 
  mutate(mean_data_value = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_data_value)) +
  geom_line(aes(color = state_abbrev)) +
  theme(legend.text = element_text(size = 8), legend.spacing.x = unit(0.05, 'cm'), 
        legend.position = 'right') +
   xlab("Year") + ylab("Mean Data Value") +
  scale_colour_discrete(name = "State Abbreviation")
```

Overall, it looks like mean data values for all the states included in the 
'spaghetti' plot fluctuated between 11.5 and 29.5 from 2002 and 2010.

I will now make a two panel plot showing, for years 2006 and 2010, the
distribution of data values for responses "Poor" to "Excellent" among locations
in NY state.

```{r, collapse = TRUE}
ny_2006_2010_df = select(brfss_smart2010_df, year, state_abbrev, state_w_county, 
                         response, data_value) %>% 
  filter(state_abbrev == "NY", year == 2006 | year == 2010)

ggplot(ny_2006_2010_df, aes(x = response, y = data_value, 
                            color = state_w_county)) +
  geom_point() +
  facet_grid(. ~ year) +
  labs(
    title = "2006 and 2010 Data Values for Responses among NY Locations",
    x = "Response Type",
    y = "Data Value",
    caption =  "Data from Instacart") +
  theme(legend.title = element_blank(), legend.text = element_text(size = 7))
```

In 2006 and 2010, the response type "Poor" had the lowest data value and was 
from Weschester County. In 2010, the response type "Very good" 
had the highest data value and was also from Weschester County. For 2006, the
response type with the highest data value was "Good" and from Queens County.

## Problem 3

First, I will load and tidy the accelerometer data.

```{r, collapse = TRUE}
accel_data_df = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day_type = case_when(
      day == "Monday" | day == "Tuesday" | day == "Wednesday" | 
        day == "Thursday" |
        day == "Friday" ~ "weekday",
      day == "Sunday" | day == "Saturday" ~ "weekend",
      TRUE ~ ""))  %>%
  mutate(day = as.factor(day)) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    values_to = "activity_number")
```

I will now create a table focusing on the total activity over the day. I will 
create a total activity variable for each day, and then create a table with 
these totals.

```{r, collapse = TRUE}
accel_data_table = group_by(accel_data_df, day) %>% 
  summarize(total_activity = sum(activity_number)) %>% 
  mutate(total_activity, rank = min_rank(desc(total_activity))) %>% 
  arrange(desc(rank)) %>% 
  select(day,total_activity) %>% 
  rename("Day of the Week" = day) %>% 
  rename("Total Activity" = total_activity) %>% 
  knitr::kable()
  
accel_data_table
```

It appears like total activity is lower on the weekends and the earlier days of 
the week compared with the later days in the week. Total activity also appears
to peak on Fridays.

I will now make a single panel plot that shows the 24-hour activity time courses
for each day and will use color to indicate day of the week.

```{r}
accel_data_plot = group_by(accel_data_df, day) %>% 
 pivot_wider(
   names_from = "activity",
   values_from = "activity_number") %>% 
  mutate('Hour 1' = sum(c_across(activity_1:activity_60)),
         'Hour 2' = sum(c_across(activity_60:activity_120)),
         'Hour 3' = sum(c_across(activity_120:activity_180)),
         'Hour 4' = sum(c_across(activity_180:activity_240)),
         'Hour 5' = sum(c_across(activity_240:activity_300)),
         'Hour 6' = sum(c_across(activity_300:activity_360)),
         'Hour 7' = sum(c_across(activity_360:activity_420)),
         'Hour 8' = sum(c_across(activity_420:activity_480)),
         'Hour 9' = sum(c_across(activity_480:activity_520)),
         'Hour 10' = sum(c_across(activity_520:activity_580)),
         'Hour 11' = sum(c_across(activity_580:activity_640)),
         'Hour 12' = sum(c_across(activity_640:activity_720)),
         'Hour 13' = sum(c_across(activity_720:activity_780)),
         'Hour 14' = sum(c_across(activity_780:activity_840)),
         'Hour 15' = sum(c_across(activity_840:activity_900)),
         'Hour 16' = sum(c_across(activity_900:activity_960)),
         'Hour 17' = sum(c_across(activity_960:activity_1020)),
         'Hour 18' = sum(c_across(activity_1020:activity_1080)),
         'Hour 19' = sum(c_across(activity_1080:activity_1140)),
         'Hour 20' = sum(c_across(activity_1140:activity_1200)),
         'Hour 21' = sum(c_across(activity_1200:activity_1260)),
         'Hour 22' = sum(c_across(activity_1260:activity_1320)),
         'Hour 23' = sum(c_across(activity_1320:activity_1380)),
         'Hour 24' = sum(c_across(activity_1380:activity_1440)),
         ) %>% 
  select(day,'Hour 1':'Hour 24') %>% 
  pivot_longer(
    'Hour 1':'Hour 24',
    names_to = "Hour",
    values_to = "activity_number") 

ggplot(accel_data_plot, aes(x = Hour, y = activity_number, color = day)) +
  geom_line(aes(group = day)) + geom_point() + 
  labs(
    title = "24 Hour Accelerometer Activity By Day of the Week",
    x = "Hour",
    y = "Activity Amount",
    caption =  "Data from the Advanced Cardiac Care Center, 
    Columbia University Medical Center") +
  theme(legend.title = element_blank(), legend.text = element_text(size = 7), 
        axis.text.x = element_text(angle = 90)) +
  scale_y_continuous(
    breaks = c(0, 50000, 100000, 150000, 200000, 250000, 300000),
    labels = c("0","50,000", "100,000", "150,000", "200,000", "250,000",
    "300,000")) +
  scale_x_discrete(limits = c('Hour 1', 'Hour 2', 'Hour 3', 'Hour 4', 'Hour 5',
                              'Hour 6', 'Hour 7', 'Hour 8', 'Hour 9', 'Hour 10',
                              'Hour 11', 'Hour 12', 'Hour 13', 'Hour 14', 
                              'Hour 15', 'Hour 16', 'Hour 17', 'Hour 18',
                              'Hour 19', 'Hour 20', 'Hour 21', 'Hour 22',
                              'Hour 23', 'Hour 24')) 
```

Overall, accelerometer activity tends to be lowest at the earliest hours of the
day. Activity tends to peak between hours 10-13 and hours 20-22. Sunday at
hour 12 has the highest activity amount, followed by Friday at hours 21 and 22.